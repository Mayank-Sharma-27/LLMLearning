### Basics of tokens
The basic unit of a language model is called as a token. A token can be anything a character, a work, a part of a word depending on the model.
The process of breaking an original text into a token is called tokenisation, for GPT4 average token is approximately 3/4 of the word.

There are 2 main types of language models

1. Masked Language models: These models are trained to predict missing tokens anywhere in the sentence using the context from both and after the missing tokens
2. Autoregressive language models : These models are trained to predict the next token in a sequence using only the preceding tokens.

For picture example check page number 5 in the book.

### Self Supervision
The reason these models are able to scale up so quickly is due to self supervision we do not need to label data which was a bottle neck.

```
Self Supervision is different from unsupervision. In self-supervised learning, lables are inferred from the input data. In un-supervised learning, you don't need labels at all.

```

### Models

The model that can work with one data modality is called multimodal, for example a text model can only work with tasks related to text. Auto models can handle speech recoginzation and speech.

![[Multimodal.excalidraw]](/Excalidraw/Multimodal.excalidraw.md)

### AI Engineering
Ai engineering refers to the process of building applications on top of foundational models.

Not much to take notes here.


This chapter was a general introduction not much to take notes here.

